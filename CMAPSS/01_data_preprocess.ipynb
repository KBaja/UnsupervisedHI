{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables==3.7.0 in /usr/local/lib/python3.8/dist-packages (3.7.0)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.8/dist-packages (from tables==3.7.0) (2.8.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tables==3.7.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from tables==3.7.0) (1.23.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tables==3.7.0) (3.0.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tables==3.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, 'CMAPSS')\n",
    "import pre_process_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth HI\n",
    "TEST = 'Data/N-CMAPSS'\n",
    "PATH_IN = TEST\n",
    "SOURCE = 'N-CMAPSS_DS03'\n",
    "\n",
    "ground_truth = pd.read_hdf(PATH_IN + \"/\" + SOURCE+'_cycle_test.h5', 'df')\n",
    "ground_truth2 = pd.read_hdf(PATH_IN + \"/\" + SOURCE+'_cycle_dev.h5', 'df')\n",
    "\n",
    "ground_truth = pd.concat([ground_truth,ground_truth2]).sort_values('unit').sort_index()\n",
    "\n",
    "\n",
    "g_units = ground_truth.unit.values\n",
    "g_his = ground_truth.HI.values\n",
    "\n",
    "g_cycles = []\n",
    "_, s_idx = np.unique(g_units, return_index=True)\n",
    "for i in g_units[np.sort(s_idx)]:\n",
    "    idx = np.ravel(g_units==i)\n",
    "    g_cycles.extend(np.arange(idx.sum())+1)\n",
    "g_cycles = np.array(g_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "XS_train (5571277, 14)\n",
      "(5571277, 1)\n"
     ]
    }
   ],
   "source": [
    "# DS003 Data\n",
    "\n",
    "\n",
    "with h5py.File(PATH_IN + \"/\" + SOURCE + '.h5', 'r') as hdf:\n",
    "        # Development set\n",
    "        W_train = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_train = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        # X_v_train = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_train = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_train = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_train = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        # X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "                             \n",
    "print('')\n",
    "# print(\"Operation time (min): \" , (time.clock()-t)/60)\n",
    "print('')\n",
    "units_train=A_train[:,0].reshape(-1,1)\n",
    "cycles_train=A_train[:,1].reshape(-1,1)\n",
    "fc_train = A_train[:,2].reshape(-1,1)\n",
    "hi_train = A_train[:,-1]\n",
    "\n",
    "units_test=A_test[:,0].reshape(-1,1)\n",
    "cycles_test=A_test[:,1].reshape(-1,1)\n",
    "fc_test = A_test[:,2].reshape(-1,1)\n",
    "hi_test = A_test[:,-1]\n",
    "print(\"XS_train\",X_s_train.shape)\n",
    "print(units_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional create OOD data\n",
    "fc_for_training = [1]\n",
    "idx_train = np.ravel(np.isin(fc_train,fc_for_training))\n",
    "idx_test = np.ravel(np.isin(fc_test,fc_for_training))\n",
    "X_s_train_fc = np.concatenate([X_s_train[idx_train],X_s_test[idx_test]])\n",
    "w_train_fc = np.concatenate([W_train[idx_train],W_test[idx_test]])\n",
    "u_train_fc = np.concatenate([units_train[idx_train],units_test[idx_test]])\n",
    "c_train_fc = np.concatenate([cycles_train[idx_train],cycles_test[idx_test]])\n",
    "y_train_fc = np.concatenate([Y_train[idx_train],Y_test[idx_test]])\n",
    "t_train_fc = np.concatenate([T_train[idx_train],T_test[idx_test]])\n",
    "hi_train_fc = np.concatenate([hi_train[idx_train],hi_test[idx_test]])\n",
    "\n",
    "idx_train = np.ravel(~np.isin(fc_train,fc_for_training))\n",
    "idx_test = np.ravel(~np.isin(fc_test,fc_for_training))\n",
    "X_s_test_fc = np.concatenate([X_s_train[idx_train],X_s_test[idx_test]])\n",
    "w_test_fc = np.concatenate([W_train[idx_train],W_test[idx_test]])\n",
    "u_test_fc = np.concatenate([units_train[idx_train],units_test[idx_test]])\n",
    "c_test_fc = np.concatenate([cycles_train[idx_train],cycles_test[idx_test]])\n",
    "y_test_fc = np.concatenate([Y_train[idx_train],Y_test[idx_test]])\n",
    "t_test_fc = np.concatenate([T_train[idx_train],T_test[idx_test]])\n",
    "hi_test_fc = np.concatenate([hi_train[idx_train],hi_test[idx_test]])\n",
    "\n",
    "X_s_train = X_s_train_fc\n",
    "W_train = w_train_fc\n",
    "units_train = u_train_fc\n",
    "cycles_train = c_train_fc\n",
    "Y_train = y_train_fc\n",
    "T_train = t_train_fc\n",
    "hi_train = hi_train_fc\n",
    "\n",
    "X_s_test = X_s_test_fc\n",
    "W_test = w_test_fc\n",
    "units_test = u_test_fc\n",
    "cycles_test = c_test_fc\n",
    "Y_test = y_test_fc\n",
    "T_test = t_test_fc\n",
    "hi_test = hi_test_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING\n",
    "# scaler_X = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_X = MinMaxScaler()\n",
    "X_s_train = scaler_X.fit_transform(X_s_train)\n",
    "X_s_test = scaler_X.transform(X_s_test)\n",
    "\n",
    "# scaler_W = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler_W = MinMaxScaler()\n",
    "W_train = scaler_W.fit_transform(W_train)\n",
    "W_test = scaler_W.transform(W_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional subsample data\n",
    "# AND float16 precision (25% memory reduction)\n",
    "sample_freq=10\n",
    "\n",
    "# # SAMPLE\n",
    "X_s_Train,W_Train,Y_Train,T_Train,units_Train,cycles_Train,hi_Train=pre_process_utils.sample_data(\n",
    "    X_s_train,W_train,Y_train,T_train,units_train,cycles_train,hi_train,sample_freq)\n",
    "\n",
    "X_s_Test,W_Test,Y_Test,T_Test,units_Test,cycles_Test,hi_Test=pre_process_utils.sample_data(\n",
    "    X_s_test,W_test,Y_test,T_test,units_test,cycles_test,hi_test,sample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n",
      "2030\n"
     ]
    }
   ],
   "source": [
    "# Full window\n",
    "X_windows, C_windows, U_windows = pre_process_utils.sequence_generator_full_trajectory(X_s_Train,units_Train,cycles_Train,2030)\n",
    "W_windows,_,_ = pre_process_utils.sequence_generator_full_trajectory(W_Train,units_Train,cycles_Train,max = X_windows.shape[1])\n",
    "Y_windows,_,_ = pre_process_utils.sequence_generator_full_trajectory(Y_Train,units_Train,cycles_Train,max = X_windows.shape[1])\n",
    "T_windows,_,_ = pre_process_utils.sequence_generator_full_trajectory(T_Train,units_Train,cycles_Train,max = X_windows.shape[1])\n",
    "\n",
    "X_windows_test, C_windows_test, U_windows_test = pre_process_utils.sequence_generator_full_trajectory(X_s_Test,units_Test,cycles_Test,X_windows.shape[1])\n",
    "W_windows_test,_,_ = pre_process_utils.sequence_generator_full_trajectory(W_Test,units_Test,cycles_Test,max = X_windows.shape[1])\n",
    "Y_windows_test,_,_ = pre_process_utils.sequence_generator_full_trajectory(Y_Test,units_Test,cycles_Test,max = X_windows.shape[1])\n",
    "T_windows_test,_,_ = pre_process_utils.sequence_generator_full_trajectory(T_Test,units_Test,cycles_Test,max = X_windows.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Window Size\n",
    "WINDOW_LEN = 50\n",
    "stride = 1\n",
    "\n",
    "X_windows, U_windows, C_windows=pre_process_utils.sequence_generator(X_s_Train,units_Train,cycles_Train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows,_,_= pre_process_utils.sequence_generator(W_Train,units_Train,cycles_Train,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows,_,_= pre_process_utils.sequence_generator(Y_Train,units_Train,cycles_Train,sequence_length=WINDOW_LEN,option='last',stride = stride)\n",
    "\n",
    "\n",
    "X_windows_test, U_windows_test,C_windows_test=pre_process_utils.sequence_generator(X_s_Test,units_Test,cycles_Test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "W_windows_test,_,_ = pre_process_utils.sequence_generator(W_Test,units_Test,cycles_Test,sequence_length=WINDOW_LEN,stride = stride)\n",
    "Y_windows_test,_,_ = pre_process_utils.sequence_generator(Y_Test,units_Test,cycles_Test,sequence_length=WINDOW_LEN,option='last',stride = stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATA\n",
    "\n",
    "np.save(PATH_IN + '/X_windows.npy',X_windows)\n",
    "np.save(PATH_IN + '/U_windows.npy',U_windows)\n",
    "np.save(PATH_IN + '/C_windows.npy',C_windows)\n",
    "np.save(PATH_IN + '/W_windows.npy',W_windows)\n",
    "np.save(PATH_IN + '/Y_windows.npy',Y_windows)\n",
    "\n",
    "\n",
    "np.save(PATH_IN + '/X_windows_test.npy',X_windows_test)\n",
    "np.save(PATH_IN + '/U_windows_test.npy',U_windows_test)\n",
    "np.save(PATH_IN + '/C_windows_test.npy',C_windows_test)\n",
    "np.save(PATH_IN + '/W_windows_test.npy',W_windows_test)\n",
    "np.save(PATH_IN + '/Y_windows_test.npy',Y_windows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
